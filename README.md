# ğŸ‘€GazeFlow

XR eyeâ€‘tracking experience that explores how fragmented visual moments can be measured, understood, and reshaped into a clearer picture in virtual space.

---

## Overview

Modern digital environments train us to stare at flat, static screens from a fixed distance. Thatâ€™s convenient for devices but not ideal for developing healthy, flexible visual systems. GazeFlow asks a simple question: **what if our digital tools actively guided the eyes to move, focus, and explore, instead of staying frozen on one spot?**

Instead of passive consumption, we use immersive VR to encourage **active visual engagement and guided eye movement**. We prototype small, playful tasks that move targets in depth, change size, and invite users to track, fixate, and readâ€”turning everyday VR time into an opportunity for healthier, more intentional visual behavior rather than just more staring.

Weâ€™re inspired by work like [Luminopia](https://www.luminopia.com/) and VRâ€‘based vision therapy, but GazeFlow is a hackathonâ€‘scale exploration, not a medical product.

---
## Team
### Team Name: PUJLY GazeFlow
"Do you see the vision"

<img width="512" height="384" alt="image" src="https://github.com/user-attachments/assets/85998768-4aae-4ab8-abf4-3b1e29d63aeb" />


- Rabiat Sadiq - Developer
Email: rabiats@andrew.cmu.edu
GitHub: https://github.com/RabiatS

- Yulkti Poddar - Product designer
Email: Ypoddar@andrew.cmu.edu
GitHub: https://github.com/Yuktipoddar

- Uma Dhamija - UI/UX Designer
Email: urd@andrew.cmu.edu
GitHub: https://github.com/Umaracheldhamija

- Priyal Shrivastava - Developer
Email: pshriva2@andrew.cmu.edu

## Vision

Because so much of kidsâ€™ visual world happens on 2D screens, they often donâ€™t get the full range of oculomotor experiencesâ€”changing depth, convergence, and rich trackingâ€”that their visual systems evolved for. Our goal isnâ€™t to eliminate screens; that ship has sailed. Itâ€™s to **design digital experiences that work with, rather than against, healthy visual development**.

GazeFlow explores how immersive XR can:

- Guide **intentional eye movement** through interactive exercises.  
- Measure where attention lands and how long it stays there.  
- Turn those tiny measurements into **â€œfragmentsâ€ of a larger mosaic** of visual function.  

Weâ€™re still early in the build, but the prototype already includes moving targets, simple â€œeyeâ€‘chartâ€ style reading, and logging tools that capture how users respond.

---

## Current Prototype Features
<img width="1222" height="811" alt="image" src="https://github.com/user-attachments/assets/1be62838-b9a1-490f-93e0-0b6452aa8196" />


### 1. Moving Focus Ball

We place a simple object (currently a ball; soon a butterfly) in front of the user, then gradually move it farther away in steps.

- The user is asked to **keep their eyes on the object** as it retreats in depth.  
- A controller script moves the object forward from the user and then back by a configurable distance every few seconds.  
- This simulates a very rough version of â€œhow far can you comfortably follow a target?â€ and hints at convergence and distanceâ€‘based performance.

### 2. Gaze Fragment Logger

We use a script called `GazeFragmentLogger` to turn â€œare they looking at it?â€ into simple, measurable fragments:

- Uses the **headâ€™s forward direction** (Quest 3) as a standâ€‘in for eye gaze for now.  
- Checks each frame whether the user is looking roughly at a target object.  
- Measures **how long they keep their gaze on that object** (fixation time).  
- Detects when they look away and resets the timer.  
- Logs to the Unity Console:
  - When they start looking  
  - How long theyâ€™ve been looking  
  - When they stop looking and the total fixation duration  

This gives us a first, concrete signal of **focus endurance** we can later export, visualize, or feed into more advanced training games and analytics.

### 3. Early Letterâ€‘Chartâ€‘Style Test (Planned / In Progress)

Weâ€™re building a VR version of familiar letterâ€‘chart tests:

- A TextMeshPro â€œchartâ€ in VR shows rows of letters (e.g., E / F P / T O Z / â€¦).  
- The chart can:
  - Move farther away from the user, and/or  
  - Shrink the letter size over time.  
- The user will use their controller ray to indicate whether they can still read a line or not.  

This isnâ€™t calibrated like a full Snellen or logMAR system; itâ€™s an **interaction pattern** we can build on: distance + clarity + user response, captured as another fragment in the mosaic.

### 4. Butterfly / Followâ€‘theâ€‘Target Path (Planned / In Progress)

Weâ€™re adding a more playful, natural targetâ€”a butterfly or similar objectâ€”that:

- Flies along a smooth, curved path in front of the user.  
- Encourages **smooth pursuit** eye movements instead of jerky jumps.  
- Can be combined with gaze logging so we can see how well users track a moving, nonâ€‘linear target.

---

## Why This Could Help

Weâ€™re not making clinical claims, but our direction is grounded in existing research and products:

- VR and eyeâ€‘tracking are already being used in pediatric ophthalmology for **assessment, therapy, and visualâ€‘field testing**, with promising results.  
- VRâ€‘based vision therapy has shown **comparable or better outcomes** than some traditional officeâ€‘based therapies in trials, especially for convergence insufficiency and amblyopia.  
- VR lazyâ€‘eye treatment products like Luminopia demonstrate that **engaging, gamified visual tasks in a headset can improve visual acuity and adherence**.

GazeFlow sits at the intersection of these ideas and hackathon feasibility: we prototype the **interaction patterns, logging tools, and UX** needed to make these kinds of experiences approachable and repeatable.

---

## How It Works (Technical Highâ€‘Level)

- **Engine:** Unity 6000 (URP)  
- **Platforms:** Meta Quest 3(non eye tracking); designed to extend to Meta Quest Pr( With eye tracking)  
- **XR SDK:** Meta XR Allâ€‘inâ€‘One / Core SDK  
- **Tracking:**
  - Currently: head direction from the XR rig / OVRCameraRig  
  - Future: eye tracking via `OVREyeGaze` on Quest Pro  

Key building blocks:

- A **moving target controller** that spawns an object in front of the user and gradually moves it farther away over time.  
- The **GazeFragmentLogger** that:
  - Detects when the user is looking at the target (angle threshold)  
  - Tracks fixation duration  
  - Logs â€œstartâ€, â€œongoingâ€, and â€œstopâ€ events to the Console  
  - Designed to save to CSV (timestamp, event type, fixation length) so fragments can be analyzed later.  
- A **response logger** (in progress) tied to simple UI buttons so users can rayâ€‘select answers (â€œCan seeâ€, â€œCanâ€™t seeâ€, or letter choices) and have them stored with the current difficulty level.

---

## Future / Planned Features

Weâ€™re still actively building. Some of the things on our nearâ€‘term roadmap:

- Swap from headâ€‘based gaze approximation to **true eye tracking** on Quest Pro.  
- Hook up CSV export for both gaze events and user responses so we can analyze patterns across sessions.  
- Build a visual **â€œmosaicâ€ summary screen** that reveals tiles as tasks are completed, and colors them based on how well the user performed.  
- Add more miniâ€‘tests: peripheral target detection, smooth pursuit along paths, divided attention tasks (two targets competing for gaze).  
- Explore simple, adaptive difficulty: if a user does well, we automatically move to harder distances or smaller targets; if they struggle, we keep things easier.

---

## Working with the Repo (For Teammates)

### Cloning the project

```bash
git clone https://github.com/RabiatS/GazeFlow.git
cd GazeFlow



# Check status
git status

# Pull latest changes
git pull origin main

# Stage all changes
git add .

# Commit with message
git commit -m "Your message here"

# Push to GitHub
git push origin main

# View commit history
git log --oneline

# Undo uncommitted changes
git checkout .
